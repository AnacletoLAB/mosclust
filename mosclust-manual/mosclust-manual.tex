\documentclass{article}
\usepackage[ae,hyper]{Rd}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

\noindent
\rule[6mm]{12.3cm}{0.5mm}


\vspace{0.5cm}
\begin{center}
\begin{minipage}{0.8\textwidth}

{\LARGE
The mosclust package: Reference Manual}\\

\noindent
\vspace{0.5 cm}
(v. 1.0 - September 2006)

\vspace{0.6cm}
{\large \em   Giorgio Valentini}

{ D.S.I. \\
Dipartimento di Scienze dell' Informazione \\
Universit\`a degli Studi di Milano\\
e-mail : valentini@dsi.unimi.it}
\end{minipage}
\end{center}

\vspace{0.5cm}
\noindent
\rule[6mm]{12.3cm}{0.5mm}

\newpage

\Rdcontents{Index}
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\HeaderA{mosclust-package}{Model order selection for clustering}{mosclust.Rdash.package}
\aliasA{mosclust}{mosclust-package}{mosclust}
\keyword{package}{mosclust-package}
\keyword{cluster}{mosclust-package}
\keyword{htest}{mosclust-package}
\begin{Description}\relax
The \emph{mosclust} R package (that stands for \bold{m}odel \bold{o}rder \bold{s}election for \bold{clust}ering problems)
implements a set of functions to discover significant structures in bio-molecular data.
Using multiple perturbations of the data the stability of clustering solutions is assessed. Different
perturbations may be used: resampling techniques, random projections and noise injection. Stability measures
for the estimate of clustering solutions and statistical tests to assess their significance are provided.
\end{Description}
\begin{Details}\relax
\Tabular{ll}{
Package: & mosclust\\
Type: & Package\\
Version: & 1.0\\
Date: & 2006-09-08\\
License: & GPL \\
}


Recently, several methods based on the concept of stability have been proposed to estimate the "optimal" number of clusters in
complex bio-molecular data. In this conceptual framework multiple clusterings are obtained by introducing
perturbations into the original data, and a clustering is considered reliable if it is approximately maintained across multiple
perturbations.

Several perturbation techniques have been proposed, ranging form bootstrap techniques, to random projections to lower dimensional
subspaces to noise injection procedures. All these perturbation techniques are implemented in \emph{mosclust}.

The library implements indices of stability/reliability of the clusterings based on the distribution of similarity measures between multiple instances of
clusterings performed on multiple instances of data obtained through a given random perturbation of the original data.

These indices provides a "score" that can be used to compare the reliability of different clusterings. 
Moreover statistical tests based on \eqn{\chi^2}{} and on the classical Bernstein inequality are implemented in order to assess the statistical
significance of the discovered clustering solutions. By this approach we could also find multiple structures simultaneously present in the data. For instance,
it is possible that data exhibit a hierarchial structure, with subclusters inside other clusters, and using the indices and the statistical tests
implemented in \emph{mosclust} we may detect them at a given significance level.

Summarizing, this package may be used for:
\Itemize{ 
\item Assessment of the reliability of a given clustering solution
\item Clustering model order selection: what about the "natural" number of clusters inside the data?
\item Assessment of the statistical significance of a given clustering solution
\item Discovery of multiple structures underlying the data: are there multiple reliable clustering solutions at a given significance level?
} 

The statistical tests implemented in the package have been designed with the theoretical and methodological contribution of
\emph{Alberto Bertoni} (DSI, Universit\`a degli Studi di Milano).
\end{Details}
\begin{Author}\relax
Giorgio Valentini <valenti@dsi.unimi.it>
\end{Author}
\begin{References}\relax
Bittner, M. et al., Molecular classification of malignant melanoma by gene expression profiling, Nature, 406:536--540, 2000.

Monti, S.,  Tamayo P.,  Mesirov J. and Golub T., Consensus Clustering: A Resampling-based Method for Class Discovery and Visualization of Gene
Expression Microarray Data, Machine Learning, 52:91--118, 2003.

Dudoit S. and Fridlyand J., A prediction-based resampling method for estimating the number of clusters in a dataset, Genome Biology,
3(7): 1-21, 2002.

Kerr M.K. and Curchill G.A.,Bootstrapping cluster analysis: assessing the reliability of conclusions from microarray experiments,
PNAS, 98:8961--8965, 2001.

McShane, L.M., Radmacher, D., Freidlin, B., Yu, R.,  Li, M.C. and Simon, R.,
Method for assessing reproducibility of clustering patterns observed in analyses of microarray data,
Bioinformatics, 11(8), pp. 1462-1469, 2002.

Ben-Hur, A. Ellisseeff, A. and Guyon, I., A stability based method for discovering structure in clustered data,
In: "Pacific Symposium on Biocomputing", Altman, R.B. et al (eds.), pp, 6-17, 2002.

Smolkin M. and Gosh D., Cluster stability scores for microarray data in cancer studies, BMC Bioinformatics,
36(4), 2003.

W. Hoeffding, Probability inequalities for sums of independent random variables, J. Amer. Statist. Assoc. vol.58 pp. 13-30, 1963.

A.Bertoni, G. Valentini, Randomized maps for assessing the reliability of patients clusters in DNA microarray data analyses, 
Artificial Intelligence in Medicine 37(2):85-109  2006

A.Bertoni, G. Valentini, Model order selection for clustered bio-molecular data,  
In: Probabilistic Modeling and Machine Learning in Structural and Systems Biology, J. Rousu, S. Kaski and E. Ukkonen (Eds.), 
Tuusula, Finland, 17-18 June,  2006

A.Bertoni, G. Valentini, Discovering significant structures in clustered data through Bernstein inequality, 
CISI '06, Conferenza Italiana Sistemi Intelligenti, Ancona, Italia, 2006.

G. Valentini, Clusterv: a tool for assessing the reliability of clusters discovered in DNA microarray data, Bioinformatics,
22(3):369-370, 2006.
\end{References}
\begin{SeeAlso}\relax
\emph{clusterv}
\end{SeeAlso}

\HeaderA{Bernstein.compute.pvalues}{Function to compute the stability indices and the p-values associated to a set of clusterings according to Bernstein inequality.}{Bernstein.compute.pvalues}
\aliasA{Bernstein.ind.compute.pvalues}{Bernstein.compute.pvalues}{Bernstein.ind.compute.pvalues}
\keyword{cluster}{Bernstein.compute.pvalues}
\keyword{htest}{Bernstein.compute.pvalues}
\begin{Description}\relax
For a given similarity matrix a list of stability indices, sorted by descending order, from the most significant
clustering to the least significant is given, and the corresponding p-values, computed according to a Bernstein inequality based test are  provided.
\end{Description}
\begin{Usage}
\begin{verbatim}
Bernstein.compute.pvalues(sim.matrix)

Bernstein.ind.compute.pvalues(sim.matrix)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{sim.matrix}] a matrix that stores the similarity between pairs of clustering across multiple number of clusters
and multiple clusterings.
Rows correspond to the different clusterings; columns to the n repeated clusterings for each number of clusters
Row 1 corresond to a 2-clustering, row 2 to a 3-clustering, ... row m to a m+1 clustering. 
\end{ldescription}
\end{Arguments}
\begin{Details}\relax
The stability index for a given clustering is computed as the mean of the similarity indices between pairs of 
k-clusterings obtained from the perturbed data. The similarity matrix given as input can be obtained from the functions
do.similarity.resampling, do.similarity.projection, do.similarity.noise. 
A list of p-values, sorted by descending order, from the most significant
clustering to the least significant is given according to a test based on Bernstein inequality. 
The test is based on the distribution of the similarity measures between pairs of clustering performed on perturbed data,
but differently from the chi-square based test (see \code{\LinkA{Chi.square.compute.pvalues}{Chi.square.compute.pvalues}}), no assumptions are made
about the "a priori" distribution of the similarity measures.
The function \code{Bernstein.ind.compute.pvalues} assumes also that the the random variables represented by the means
of the similarities between pairs of clusterings are independent, while, on the contrary, 
the function \code{Bernstein.compute.pvalues} no assumptions are made.
Low p-value mean that there is a significant difference between the 
top sorted and the given clustering. Please, see the papers cited in the reference section for more technical details.
\end{Details}
\begin{Value}
a list with 4 components:
\begin{ldescription}
\item[\code{ordered.clusterings }] a vector with the number of clusters ordered from the most significant to the least significant
\item[\code{p.value }] a vector with the corresponding p-values computed according to Bernstein inequality and Bonferroni correction
in descending order (theri values correspond to the clusterings of the vector ordered.clusterings)
\item[\code{means }] vector with the mean similarity for each clustering
\item[\code{variance }] vector with the variance of the similarity for each clustering
\end{ldescription}
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{References}\relax
W. Hoeffding, Probability inequalities for sums of independent random variables, J. Amer. Statist. Assoc. vol.58 pp. 13-30, 1963.

A.Bertoni, G. Valentini, Discovering significant structures in clustered data through Bernstein inequality, 
CISI '06, Conferenza Italiana Sistemi Intelligenti, Ancona, Italia, 2006.
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{Chi.square.compute.pvalues}{Chi.square.compute.pvalues}}, \code{\LinkA{Hypothesis.testing}{Hypothesis.testing}},

\code{\LinkA{do.similarity.resampling}{do.similarity.resampling}}, \code{\LinkA{do.similarity.projection}{do.similarity.projection}}, \code{\LinkA{do.similarity.noise}{do.similarity.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Computation of the p-values according to Bernstein inequality using 
# resampling techniques and a hierarcchical clustering algorithm
M <- generate.sample.h2 (n=20, l=10, Delta.h=4, Delta.v=2, sd=0.15);
S.HC <- do.similarity.resampling (M, c=15, nsub=20, f=0.8, s=sFM, 
                           alg.clust.sim=Hierarchical.sim.resampling);
# Bernstein test with no assumption of independence
Bernstein.compute.pvalues(S.HC)
# Bernstein test with  assumption of independence
Bernstein.ind.compute.pvalues(S.HC)
\end{ExampleCode}
\end{Examples}

\HeaderA{Bernstein.p.value}{Function to compute the p-value according to Bernstein inequality.}{Bernstein.p.value}
\keyword{cluster}{Bernstein.p.value}
\keyword{htest}{Bernstein.p.value}
\begin{Description}\relax
The Bernstein inequality gives an upper bound to the probability that the means of two random variables differ by chance, considering also their variance.
This function implements the Berstein inequality and it is used by the functions \code{Bernstein.compute.pvalues} and
\code{Bernstein.ind.compute.pvalues}
\end{Description}
\begin{Usage}
\begin{verbatim}
Bernstein.p.value(n, Delta, v)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] number of observations of the random variable 
\item[\code{Delta}] difference between the means 
\item[\code{v}] variance of the random variable 
\end{ldescription}
\end{Arguments}
\begin{Value}
a real number that provides an upper bound to the probability that the two means differ by chance
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{References}\relax
W. Hoeffding, Probability inequalities for sums of independent random variables, J. Amer. Statist. Assoc. vol.58 pp. 13-30, 1963.
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{Bernstein.compute.pvalues}{Bernstein.compute.pvalues}}, \code{\LinkA{Bernstein.ind.compute.pvalues}{Bernstein.ind.compute.pvalues}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Computation of the upper bounds to the probability that the two means differ by chance
Bernstein.p.value(n=100, Delta=0.1, v=0.01)
Bernstein.p.value(n=100, Delta=0.05, v=0.01)
Bernstein.p.value(n=100, Delta=0.05, v=0.1)
Bernstein.p.value(n=1000, Delta=0.05, v=0.1)
\end{ExampleCode}
\end{Examples}

\HeaderA{Chi.square.compute.pvalues}{Function to compute the stability indices and the p-values associated to a set of clusterings according to the chi-square test between 
multiple proportions.}{Chi.square.compute.pvalues}
\keyword{cluster}{Chi.square.compute.pvalues}
\keyword{htest}{Chi.square.compute.pvalues}
\begin{Description}\relax
For a given similarity matrix a list of stability indices, sorted by descending order, from the most significant
clustering to the least significant is given. Moreover the corresponding p-values, computed according to a chi-square based test are provided.
\end{Description}
\begin{Usage}
\begin{verbatim}
Chi.square.compute.pvalues(sim.matrix, s0 = 0.9)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{sim.matrix}] a matrix that stores the similarity between pairs of clustering across multiple number of clusters
and multiple clusterings. Rows correspond to the different clusterings; columns to the n repeated clusterings for each number of clusters
Row 1 correspond to a 2-clustering, row 2 to a 3-clustering, ... row m to a m+1 clustering. 
\item[\code{s0}] threshold for the similarity value (default 0.9) 
\end{ldescription}
\end{Arguments}
\begin{Details}\relax
The stability index for a given clustering is computed as the mean of the similarity indices between pairs of 
k-clusterings obtained from the perturbed data. The similarity matrix given as input can be obtained from the functions
do.similarity.resampling, do.similarity.projection, do.similarity.noise. For each k-clustering the proportion
of pairs of perturbed clusterings having similarity indices larger than a given threshold (the parameter \eqn{s0}{}) is computed.
The p-values are obtained according the chi-square test between multiple proportions (each proportion corresponds to a different k-clustering) 
A low p-value means that there is a significant difference between the top sorted and the given k-clustering.
\end{Details}
\begin{Value}
a data frame with 4 components:
\begin{ldescription}
\item[\code{ordered.clusterings }] a vector with the number of clusters ordered from the most significant to the least significant
\item[\code{p.value }] a vector with the corresponding p-values computed according to chi-square test between multiple proportions
in descending order (their values correspond to the clusterings of the vector ordered.clusterings)
\item[\code{means }] vector with the stability index (mean similarity) for each k-clustering
\item[\code{variance }] vector with the variance of the similarity for each k-clustering
\end{ldescription}
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valentini@dsi.unimi.it}
\end{Author}
\begin{References}\relax
A.Bertoni, G. Valentini, Model order selection for clustered bio-molecular data,  
In: Probabilistic Modeling and Machine Learning in Structural and Systems Biology, J. Rousu, S. Kaski and E. Ukkonen (Eds.), 
Tuusula, Finland, 17-18 June,  2006
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{Bernstein.compute.pvalues}{Bernstein.compute.pvalues}}, \code{\LinkA{Hypothesis.testing}{Hypothesis.testing}},

\code{\LinkA{do.similarity.resampling}{do.similarity.resampling}}, \code{\LinkA{do.similarity.projection}{do.similarity.projection}}, \code{\LinkA{do.similarity.noise}{do.similarity.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=10, m=15, dim=800, d=3, s=0.2)
nsubsamples <- 10;  # number of pairs od clusterings to be evaluated
max.num.clust <- 6; # maximum number of cluster to be evaluated
fract.resampled <- 0.8; # fraction of samples to subsampled
# building a similarity matrix using resampling methods, considering clusterings 
# from 2 to 10 clusters with the k-means algorithm
Sr.Kmeans.sample6 <- do.similarity.resampling(M, c=max.num.clust, nsub=nsubsamples, 
                     f=fract.resampled, s=sFM, alg.clust.sim=Kmeans.sim.resampling);
# computing p-values according to the chi square-based test
dr.Kmeans.sample6 <- Chi.square.compute.pvalues(Sr.Kmeans.sample6);
# the same, using noise to perturbate the data and  hierarchical clustering algorithm
Sn.HC.sample6 <- do.similarity.noise(M, c=max.num.clust, nnoisy=nsubsamples, perc=0.5, 
                                 s=sFM, alg.clust.sim=Hierarchical.sim.noise);
dn.HC.sample6 <- Chi.square.compute.pvalues(Sn.HC.sample6);
\end{ExampleCode}
\end{Examples}

\HeaderA{Compute.Chi.sq}{Function to evaluate if a set of similarity distributions significantly differ using the chi square test.}{Compute.Chi.sq}
\keyword{cluster}{Compute.Chi.sq}
\keyword{htest}{Compute.Chi.sq}
\begin{Description}\relax
The set of similarity values for a specific value of k (number of clusters) are subdivided in two groups
choosing a threshold for the similarity value (default 0.9). Then different sets are compared using the chi squared test for multiple proportions.
The number of degrees of freedom are equal to the number of the different sets minus 1.
This function is iteratively used by \code{Chi.square.compute.pvalues}.
\end{Description}
\begin{Usage}
\begin{verbatim}
Compute.Chi.sq(M, s0 = 0.9)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{M}] matrix representing the similarity values for different number of clusters. Each row represents similarity values for a
number of clusters. Number of rows ==> how many numbers of clusters are considered; 
number of columns ==> cardinality of the similarity values for a given number of clusters 
\item[\code{s0}] threshold for the similarity value (default 0.9) 
\end{ldescription}
\end{Arguments}
\begin{Value}
p-value (type I error) associated with the null hypothesis (no difference between the considered set of k-clusterings)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valentini@dsi.unimi.it}
\end{Author}
\begin{References}\relax
A.Bertoni, G. Valentini, Model order selection for clustered bio-molecular data,  
In: Probabilistic Modeling and Machine Learning in Structural and Systems Biology, J. Rousu, S. Kaski and E. Ukkonen (Eds.), 
Tuusula, Finland, 17-18 June,  2006
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{Chi.square.compute.pvalues}{Chi.square.compute.pvalues}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=10, m=15, dim=800, d=3, s=0.2)
# computing the similarity matrix using random projections and hierarchcial clustering
Sim <- do.similarity.projection(M, c=6, nprojections=20, dim=JL.predict.dim(60,epsilon=0.2))
# Evaluating the p-value for the group of the 5 clusterings (from 2 to 6 clusters)
Compute.Chi.sq(Sim)
# the same, considering only the clusterings wih 2 and 6 clusters:
Compute.Chi.sq(Sim[c(1,5),])
\end{ExampleCode}
\end{Examples}

\HeaderA{compute.cumulative.multiple}{Function to compute the empirical cumulative distribution function (ECDF) of the similarity measures.}{compute.cumulative.multiple}
\aliasA{cumulative.values}{compute.cumulative.multiple}{cumulative.values}
\keyword{cluster}{compute.cumulative.multiple}
\begin{Description}\relax
The function \code{compute.cumulative.multiple} computes the empirical cumulative distribution function (ECDF) of the similarity measures 
for different number of clusters between clusterings.
The function \code{cumulative.values} returns the values of the empirical cumulative distribution
\end{Description}
\begin{Usage}
\begin{verbatim}
compute.cumulative.multiple(sim.matrix)

cumulative.values(F)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{sim.matrix}] a matrix that stores the similarity between pairs of clustering across multiple number of clusters
and multiple clusterings. Each row corresponds to a different number of clusters; number of columns
equal to  the number of subsamples considered for each number of clusters. 
\item[\code{F}] Function of class ecdf  that stores the discrete values of the cumulative distribution 
\end{ldescription}
\end{Arguments}
\begin{Value}
Function  \code{compute.cumulative.multiple}: a list of function of class ecdf.

Function  \code{cumulative.values}:      a list with two elements: the "x" element stores a vector with the values of the random variable for
which the cumulative distribution needs to be computed; the "y" element stores a vector with the corresponding
values of the cumulative distribution (i.e. y = F(x)).
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{plot.cumulative.multiple}{plot.cumulative.multiple}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=1000, d=3, s=0.2);
# generation of multiple similarity measures by resampling
Sr.kmeans.sample6 <- do.similarity.resampling(M, c=10, nsub=20, f=0.8, s=sFM, 
                                      alg.clust.sim=Kmeans.sim.resampling); 
# computation of multiple ecdf (from 2 to 10 clusters)
list.F <- compute.cumulative.multiple(Sr.kmeans.sample6);
# values of the ecdf for 8 clusters 
l <- cumulative.values(list.F[[7]])
\end{ExampleCode}
\end{Examples}

\HeaderA{compute.integral}{Functions to compute the integral of the ecdf of the similarity values}{compute.integral}
\aliasA{compute.integral.from.similarity}{compute.integral}{compute.integral.from.similarity}
\keyword{cluster}{compute.integral}
\begin{Description}\relax
The function \code{compute.integral} computes the integral of the ecdf form the function of 
class ecdf  that stores the discrete values of the empirical cumulative distribution, while
the function \code{compute.integral.from.similarity} computes the integral of the ecdf
exploiting then empirical mean of the similarity values (see the paper cited in the reference section for details).
\end{Description}
\begin{Usage}
\begin{verbatim}
compute.integral(F, subdivisions = 1000)

compute.integral.from.similarity(sim.matrix)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{F}] Function of class ecdf  that stores the discrete values of the empirical cumulative distribution 
\item[\code{subdivisions}] maximum number of subintervals used by the integration process 
\item[\code{sim.matrix}] a matrix that stores the similarity between pairs of clustering across multiple number of clusters
and multiple clusterings performed on subsamples of the original data.
Number or rows equal to the different numbers of clusters considered; number of columns      
equal to  the number of subsamples considered for each number of clusters. 
\end{ldescription}
\end{Arguments}
\begin{Value}
The function \code{compute.integral} returns the value of the estimate integral.
The function \code{compute.integral.from.similarity} returns a vector of the values of the estimate integrals (one for each row of \eqn{sim.matrix}{}).
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{References}\relax
A.Bertoni, G. Valentini, Discovering significant structures in clustered data through Bernstein inequality, 
CISI '06, Conferenza Italiana Sistemi Intelligenti, Ancona, Italia, 2006.
\end{References}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=1000, d=3, s=0.2);
# generation of multiple similarity measures by resampling
Sr.kmeans.sample6 <- do.similarity.resampling(M, c=10, nsub=20, f=0.8, s=sFM, 
                                      alg.clust.sim=Kmeans.sim.resampling); 
# computation of multiple ecdf (from 2 to 10 clusters)
list.F <- compute.cumulative.multiple(Sr.kmeans.sample6);
# computation of the integral of the ecdf with 2 clusters
compute.integral(list.F[[1]])
# computation of the integral of the ecdf with 8 clusters
compute.integral(list.F[[7]])
# computation of the integral of the ecdfs from 2 to 10 clusters
compute.integral.from.similarity(Sr.kmeans.sample6)
\end{ExampleCode}
\end{Examples}

\HeaderA{Do.boolean.membership.matrix}{Function to compute and build up a pairwise boolean membership matrix.}{Do.boolean.membership.matrix}
\keyword{cluster}{Do.boolean.membership.matrix}
\begin{Description}\relax
It computes the pairwise membership matrix for a given clustering. The number of rows is equal to the number of columns (the number of examples).
The element \eqn{m_{ij}}{} is set to 1 if the examples \eqn{i}{} and \eqn{j}{} belong to the same cluster, otherwise to 0. 
This function may be used also with clusterings that do not define strictly a partition of the data and using
diferent number of clusters for each clustering.
\end{Description}
\begin{Usage}
\begin{verbatim}
Do.boolean.membership.matrix(cl, dim.M, examplelabels)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{cl}] a clustering (list of vectors defining a clustering) 
\item[\code{dim.M}] dimension of the similarity matrix (number of examples) 
\item[\code{examplelabels}] labels of the examples drawn from the original data 
\end{ldescription}
\end{Arguments}
\begin{Value}
the pairwise boolean membership square matrix.
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation (3 clusters with 20 examples for each cluster)
M <- generate.sample3(n=20, m=2)
# k-means clustering with 3 clusters
r<-kmeans(t(M), c=3, iter.max = 1000);
# this function is implemented in the clusterv package:
cl <- Transform.vector.to.list(r$cluster); 
# generation of boolean membership square matrix:
B <- Do.boolean.membership.matrix(cl, 60, 1:60)
\end{ExampleCode}
\end{Examples}

\HeaderA{do.similarity.noise}{Function that computes sets of similarity indices using using injection of gaussian noise.}{do.similarity.noise}
\keyword{cluster}{do.similarity.noise}
\begin{Description}\relax
This function may use different clustering algorithms and different similarity measures to compute similarity indices.
Injection of gaussian noise  is applied to perturb the data. 
The gaussian noise  added to the data has 0 mean and the standard deviation is estimated from the data (it is 
set to a given percentile value of the standard deviations computed for each variable).
More precisely pairs of data sets are perturbed with noise 
and then are clustered and the resulting clusterings are compared using similarity indices between pairs of clusterings
(e.g. Rand Index, Jaccard or Fowlkes and Mallows indices). These indices are computed multiple times for different number of clusters.
\end{Description}
\begin{Usage}
\begin{verbatim}
do.similarity.noise(X, c = 2, nnoisy = 100, perc = 0.5, seed = 100, s = sFM, 
alg.clust.sim = Hierarchical.sim.noise, distance = "euclidean", hmethod = "ward")
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns)
\item[\code{c}] if it is a vector of length 1, number of clusters from 2 to c are considered; otherwise are considered 
the number of clusters stored in the vector c. 
\item[\code{nnoisy}] number of pairs of noisy data 
\item[\code{perc}] percentile of the standard deviations to be used for the added gaussian noise (default: median) 
\item[\code{seed}] numerical seed for the random generator 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows)
\item[\code{alg.clust.sim}] method that computes the similarity indices using subsampling techniques and a specific clustering algorithm. 
It may be one of the following: 
- Hierarchical.sim.resampling (hierarchical clustering algorithm, default)
- Kmeans.sim.resampling (c - mean algorithm)
- PAM.sim.resampling (Prediction Around Medoid algorithm)
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] the agglomeration method to be used. This parameter is used only by the hierarchical clustering algorithm.
This should be one of the following:
"ward", "single", "complete", "average", "mcquitty", "median" or "centroid", according of the hclust
method of the package stats. 
\end{ldescription}
\end{Arguments}
\begin{Value}
a matrix that stores the similarity between pairs of clustering across multiple number of clusters
and multiple clusterings performed on subsamples of the original data.
Number or rows equal to the length of c (number of clusters); number of columns      
equal to nsub, that is the number of subsamples considered for each number of clusters.
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{References}\relax
McShane, L.M., Radmacher, D., Freidlin, B., Yu, R.,  Li, M.C. and Simon, R.,
Method for assessing reproducibility of clustering patterns observed in analyses of microarray data,
Bioinformatics, 11(8), pp. 1462-1469, 2002.
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{do.similarity.projection}{do.similarity.projection}}, \code{\LinkA{do.similarity.resampling}{do.similarity.resampling}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing similarity indices with the fuzzy c-mean algorithm
Sn.Fuzzy.kmeans.sample6 <- do.similarity.noise(M, c=8, nnoisy=30, perc=0.5, s=sFM, 
                                      alg.clust.sim=Fuzzy.kmeans.sim.noise);
# computing similarity indices using the c-mean algorithm
Sn.Fuzzy.kmeans.sample6 <- do.similarity.noise(M, c=8, nnoisy=30, perc=0.5, s=sFM, 
                                      alg.clust.sim=Fuzzy.kmeans.sim.noise);
# computing similarity indices using the hierarchical clustering algorithm
Sn.HC.sample6 <- do.similarity.noise(M, c=8, nnoisy=30, perc=0.5, s=sFM, 
                                      alg.clust.sim=Hierarchical.sim.noise);
\end{ExampleCode}
\end{Examples}

\HeaderA{do.similarity.projection}{Function that computes sets of similarity indices using randomized maps.}{do.similarity.projection}
\keyword{cluster}{do.similarity.projection}
\begin{Description}\relax
This function may use different clustering algorithms and different similarity measures to compute similarity indices.
Random projections techniques are applied to perturb the data. 
More precisely pairs of data sets are projected into lower dimensional subspaces using randomized maps,
and then are clustered and the resulting clusterings are compared using similarity indices between pairs of clusterings
(e.g. Rand Index, Jaccard or Fowlkes and Mallows indices). These indices are computed multiple times for different number of clusters.
\end{Description}
\begin{Usage}
\begin{verbatim}
do.similarity.projection(X, c = 2, nprojections = 100, dim = 2, pmethod = "PMO", 
scale = TRUE, seed = 100, s = sFM, alg.clust.sim = Hierarchical.sim.projection, 
distance = "euclidean", hmethod = "ward")
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns)
\item[\code{c}] if it is a vector of length 1, number of clusters from 2 to c are considered; otherwise are considered 
the number of clusters stored in the vector c. 
\item[\code{nprojections}] number of pairs of projected data 
\item[\code{dim}] dimension of the projected data 
\item[\code{pmethod}] pmethod : projection method. It must be one of the following:  
"RS" (random subspace projection)
"PMO" (Plus Minus One random projection) (default)
"Norm" (normal random projection)
"Achlioptas" (Achlioptas random projection)
\item[\code{scale}] if TRUE randomized projections are scaled (default) 
\item[\code{seed}] numerical seed for the random generator 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows)
\item[\code{alg.clust.sim}] method that computes the similarity indices using subsampling techniques and a specific clustering algorithm. 
It may be one of the following: 
- Hierarchical.sim.resampling (hierarchical clustering algorithm, default)
- Kmeans.sim.resampling (c - mean algorithm)
- PAM.sim.resampling (Prediction Around Medoid algorithm)
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] the agglomeration method to be used. This parameter is used only by the hierarchical clustering algorithm.
This should be one of the following:
"ward", "single", "complete", "average", "mcquitty", "median" or "centroid", according of the hclust
method of the package stats. 
\end{ldescription}
\end{Arguments}
\begin{Value}
a matrix that stores the similarity between pairs of clustering across multiple number of clusters
and multiple clusterings performed on subsamples of the original data.
Number or rows equal to the length of c (number of clusters); number of columns      
equal to nsub, that is the number of subsamples considered for each number of clusters.
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{References}\relax
A.Bertoni, G. Valentini, Randomized maps for assessing the reliability of patients clusters in DNA microarray data analyses, 
Artificial Intelligence in Medicine 37(2):85-109  2006
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{do.similarity.resampling}{do.similarity.resampling}}, \code{\LinkA{do.similarity.noise}{do.similarity.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing similarity indices with the fuzzy c-mean algorithm
Sp.fuzzy.kmeans.sample6 <- do.similarity.projection(M, c=8, nprojections=30, 
   dim=JL.predict.dim(120,0.2), pmethod="PMO", alg.clust.sim=Fuzzy.kmeans.sim.projection);
# computing similarity indices using the c-mean algorithm
Sp.kmeans.sample6 <- do.similarity.projection(M, c=8, nprojections=30, 
   dim=JL.predict.dim(120,0.2), pmethod="PMO", alg.clust.sim=Kmeans.sim.projection);
# computing similarity indices using the hierarchical clustering algorithm
Sp.HC.sample6 <- do.similarity.projection(M, c=8, nprojections=30, 
   dim=JL.predict.dim(120,0.2), pmethod="PMO", alg.clust.sim=Hierarchical.sim.projection);
\end{ExampleCode}
\end{Examples}

\HeaderA{do.similarity.resampling}{Function that computes sets of similarity indices using resampling techniques.}{do.similarity.resampling}
\keyword{cluster}{do.similarity.resampling}
\begin{Description}\relax
This function may use different clustering algorithms and different similarity measures to compute similarity indices.
Subsampling techniques are applied to perturb the data. More precisely pairs of data sets are sampled according to an uniform distribution
without replacement and then are clustered and the resulting clusterings are compared using similarity indices between pairs of clusterings
(e.g. Rand Index, Jaccard or Fowlkes and Mallows indices). These indices are computed multiple times for different number of clusters.
\end{Description}
\begin{Usage}
\begin{verbatim}
do.similarity.resampling(X, c = 2, nsub = 100, f = 0.8, s = sFM, 
alg.clust.sim = Hierarchical.sim.resampling, distance = "euclidean", hmethod = "ward")
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns)
\item[\code{c}] if it is a vector of length 1, number of clusters from 2 to c are considered; otherwise are considered 
the number of clusters stored in the vector c. 
\item[\code{nsub}] number of pairs of subsamples 
\item[\code{f}] fraction of the data resampled without replacement 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows)
\item[\code{alg.clust.sim}] method that computes the similarity indices using subsampling techniques and a specific clustering algorithm. 
It may be one of the following: 
- Hierarchical.sim.resampling (hierarchical clustering algorithm, default)
- Kmeans.sim.resampling (c - mean algorithm)
- PAM.sim.resampling (Prediction Around Medoid algorithm)
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] the agglomeration method to be used. This parameter is used only by the hierarchical clustering algorithm.
This should be one of the following:
"ward", "single", "complete", "average", "mcquitty", "median" or "centroid", according of the hclust
method of the package stats. 
\end{ldescription}
\end{Arguments}
\begin{Value}
a matrix that stores the similarity between pairs of clustering across multiple number of clusters
and multiple clusterings performed on subsamples of the original data.
Number or rows equal to the length of c (number of clusters); number of columns      
equal to nsub, that is the number of subsamples considered for each number of clusters.
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{References}\relax
Ben-Hur, A. Ellisseeff, A. and Guyon, I., A stability based method for discovering structure in clustered data,
In: "Pacific Symposium on Biocomputing", Altman, R.B. et al (eds.), pp, 6-17, 2002.
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{do.similarity.projection}{do.similarity.projection}}, \code{\LinkA{do.similarity.noise}{do.similarity.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing similarity indices with the fuzzy c-mean algorithm
Sr.Fuzzy.kmeans.sample6 <- do.similarity.resampling(M, c=8, nsub=30, f=0.8, s=sFM, 
                           alg.clust.sim=Fuzzy.kmeans.sim.resampling);
# computing similarity indices using the c-mean algorithm
Sr.Kmeans.sample6 <- do.similarity.resampling(M, c=8, nsub=30, f=0.8, s=sFM, 
                                      alg.clust.sim=Kmeans.sim.resampling)
# computing similarity indices using the hierarchical clustering algorithm
Sr.HC.sample6 <- do.similarity.resampling(M, c=8, nsub=30, f=0.8, s=sFM);
\end{ExampleCode}
\end{Examples}

\HeaderA{Fuzzy.kmeans.sim.noise}{Function to compute similarity indices using noise injection techniques and fuzzy c-mean clustering.}{Fuzzy.kmeans.sim.noise}
\keyword{cluster}{Fuzzy.kmeans.sim.noise}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with random noise is computed for a given number of clusters. 
The variance of the added gaussian noise, estimated from the data as the perc percentile of the standard deviations of the input variables,
the percentile itself and the similarity measure can be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Fuzzy.kmeans.sim.noise(X, c = 2, nnoisy = 100, perc = 0.5, s = sFM, 
                       distance = "euclidean", hmethod = NULL)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nnoisy}] number of pairs of noisy data 
\item[\code{perc}] percentile of the standard deviations to be used for the added gaussian noise (def. 0.5) 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nnoisy)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Fuzzy.kmeans.sim.projection}{Fuzzy.kmeans.sim.projection}}, \code{\LinkA{Fuzzy.kmeans.sim.resampling}{Fuzzy.kmeans.sim.resampling}}, \code{\LinkA{perturb.by.noise}{perturb.by.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Fuzzy.kmeans.sim.noise(M, c = 2, nnoisy = 20,  s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Fuzzy.kmeans.sim.noise(M, c = 3, nnoisy = 20,  s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Fuzzy.kmeans.sim.noise(M, c = 2, nnoisy = 20,  s = sJaccard)
# 2 clusters using 0.95 percentile (more noise)
v095 <- Fuzzy.kmeans.sim.noise(M, c = 2, nnoisy = 20,  s = sFM, perc=0.95)
\end{ExampleCode}
\end{Examples}

\HeaderA{Fuzzy.kmeans.sim.projection}{Function to compute similarity indices using random projections and fuzzy c-mean clustering.}{Fuzzy.kmeans.sim.projection}
\keyword{cluster}{Fuzzy.kmeans.sim.projection}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with random projections is computed for a given number of clusters. 
The dimension of the projected data, the type of randomized map and the similarity measure may be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Fuzzy.kmeans.sim.projection(X, c = 2, nprojections = 100, dim = 2, pmethod = "PMO", 
scale = TRUE, seed = 100, s = sFM, distance = "euclidean", hmethod = NULL)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nprojections}] number of pairs of projected data 
\item[\code{dim}] dimension of the projected data 
\item[\code{pmethod}] projection method. It must be one of the following:  
RS" (random subspace projection)
PMO" (Plus Minus One random projection)
Norm" (normal random projection)
Achlioptas" (Achlioptas random projection)
\item[\code{scale}] if TRUE randomized projections are scaled (default) 
\item[\code{seed}] numerical seed for the random generator 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation)
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nprojections)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Fuzzy.kmeans.sim.resampling}{Fuzzy.kmeans.sim.resampling}}, \code{\LinkA{Fuzzy.kmeans.sim.noise}{Fuzzy.kmeans.sim.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Fuzzy.kmeans.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                                  pmethod = "PMO", s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Fuzzy.kmeans.sim.projection(M, c = 3, nprojections = 20, dim = 200, 
                                  pmethod = "PMO", s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Fuzzy.kmeans.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                                   pmethod = "PMO", s = sJaccard)
\end{ExampleCode}
\end{Examples}

\HeaderA{Fuzzy.kmeans.sim.resampling}{Function to compute similarity indices using resampling techniques and fuzzy c-mean clustering.}{Fuzzy.kmeans.sim.resampling}
\keyword{cluster}{Fuzzy.kmeans.sim.resampling}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with resampling techniques is computed for a given number of clusters,
using the fuzzy c-mean algorithm.
The fraction of the resampled data (without replacement) and the similarity measure can be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Fuzzy.kmeans.sim.resampling(X, c = 2, nsub = 100, f = 0.8, s = sFM, 
                            distance = "euclidean", hmethod = NULL)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nsub}] number of subsamples 
\item[\code{f}] fraction of the data resampled without replacement 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nsub)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Fuzzy.kmeans.sim.projection}{Fuzzy.kmeans.sim.projection}}, \code{\LinkA{Fuzzy.kmeans.sim.noise}{Fuzzy.kmeans.sim.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Fuzzy.kmeans.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Fuzzy.kmeans.sim.resampling(M, c = 3, nsub = 20, f = 0.8, s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Fuzzy.kmeans.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sJaccard)
\end{ExampleCode}
\end{Examples}

\HeaderA{Hierarchical.sim.noise}{Function to compute similarity indices using noise injection techniques and hierarchical clustering.}{Hierarchical.sim.noise}
\keyword{cluster}{Hierarchical.sim.noise}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with random noise is computed for a given number of clusters. 
The variance of the added gaussian noise, estimated from the data as the perc percentile of the standard deviations of the input variables,
the percentile itself, the similarity measure and  the type of hierarchical clustering may be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Hierarchical.sim.noise(X, c = 2, nnoisy = 100, perc = 0.5, s = sFM, 
                       distance = "euclidean", hmethod = "ward")
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nnoisy}] number of pairs of noisy data 
\item[\code{perc}] percentile of the standard deviations to be used for the added gaussian noise (def. 0.5) 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] the agglomeration method to be used. This parameter is used only by the hierarchical clustering algorithm.
This should be one of the following:
"ward", "single", "complete", "average", "mcquitty", "median" or "centroid", according of the hclust
method of the package stats. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nnoisy)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Hierarchical.sim.projection}{Hierarchical.sim.projection}}, \code{\LinkA{Hierarchical.sim.resampling}{Hierarchical.sim.resampling}}, \code{\LinkA{perturb.by.noise}{perturb.by.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Hierarchical.sim.noise(M, c = 2, nnoisy = 20,  s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Hierarchical.sim.noise(M, c = 3, nnoisy = 20,  s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Hierarchical.sim.noise(M, c = 2, nnoisy = 20,  s = sJaccard)
#  2 clusters using the Jaccard index and Pearson correlation
v2JP <- Hierarchical.sim.noise(M, c = 2, nnoisy = 20, s = sJaccard, distance="pearson")
# 2 clusters using 0.95 percentile (more noise)
v095 <- Hierarchical.sim.noise(M, c = 2, nnoisy = 20,  s = sFM, perc=0.95)
\end{ExampleCode}
\end{Examples}

\HeaderA{Hierarchical.sim.projection}{Function to compute similarity indices using random projections and hierarchical clustering.}{Hierarchical.sim.projection}
\keyword{cluster}{Hierarchical.sim.projection}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with random projections is computed for a given number of clusters. 
The dimension of the projected
data, the type of randomized map, the similarity measure and  the type of hierarchical clustering may be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Hierarchical.sim.projection(X, c = 2, nprojections = 100, dim = 2, pmethod = "RS", 
scale = TRUE, seed = 100, s = sFM, distance = "euclidean", hmethod = "ward")
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nprojections}] number of pairs of projected data 
\item[\code{dim}] dimension of the projected data 
\item[\code{pmethod}] projection method. It must be one of the following:  
RS" (random subspace projection)
PMO" (Plus Minus One random projection)
Norm" (normal random projection)
Achlioptas" (Achlioptas random projection)
\item[\code{scale}] if TRUE randomized projections are scaled (default) 
\item[\code{seed}] numerical seed for the random generator 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] the agglomeration method to be used. This parameter is used only by the hierarchical clustering algorithm.
This should be one of the following:
"ward", "single", "complete", "average", "mcquitty", "median" or "centroid", according of the hclust
method of the package stats. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nprojections)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Hierarchical.sim.resampling}{Hierarchical.sim.resampling}}, \code{\LinkA{Hierarchical.sim.noise}{Hierarchical.sim.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Hierarchical.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                                  pmethod = "PMO", s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Hierarchical.sim.projection(M, c = 3, nprojections = 20, dim = 200, 
                                  pmethod = "PMO", s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Hierarchical.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                                   pmethod = "PMO", s = sJaccard)
#  2 clusters using the Jaccard index and Pearson correlation
v2JP <- Hierarchical.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                                    pmethod = "PMO", s = sJaccard, distance="pearson")
\end{ExampleCode}
\end{Examples}

\HeaderA{Hierarchical.sim.resampling}{Function to compute similarity indices using resampling techniques and hierarchical clustering.}{Hierarchical.sim.resampling}
\keyword{cluster}{Hierarchical.sim.resampling}
\begin{Description}\relax
Function to compute similarity indices using resampling techniques and hierarchical clustering.
A vector of similarity measures between pairs of clusterings perturbed with resampling techniques is computed for a given number of clusters. 
The fraction of the resampled data (without replacement), the similarity measure and  the type of hierarchical clustering may be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Hierarchical.sim.resampling(X, c = 2, nsub = 100, f = 0.8, s = sFM, 
                            distance = "euclidean", hmethod = "ward")
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nsub}] number of subsamples 
\item[\code{f}] fraction of the data resampled without replacement 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] the agglomeration method to be used. This parameter is used only by the hierarchical clustering algorithm.
This should be one of the following:
"ward", "single", "complete", "average", "mcquitty", "median" or "centroid", according of the hclust
method of the package stats. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nsub)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Hierarchical.sim.projection}{Hierarchical.sim.projection}}, \code{\LinkA{Hierarchical.sim.noise}{Hierarchical.sim.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Hierarchical.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Hierarchical.sim.resampling(M, c = 3, nsub = 20, f = 0.8, s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Hierarchical.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sJaccard)
#  2 clusters using the Jaccard index and Pearson correlation
v2JP <- Hierarchical.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sJaccard, 
                                    distance="pearson")
\end{ExampleCode}
\end{Examples}

\HeaderA{Hybrid.testing}{Statistical test based on stability methods for model order selection.}{Hybrid.testing}
\keyword{cluster}{Hybrid.testing}
\keyword{htest}{Hybrid.testing}
\begin{Description}\relax
Statistical test to estimate if there is a significant difference between a set of clustering solutions.
Given a set of clustering solutions (that is solutions for different number \emph{k} of clusters),
the statistical test using both the \emph{Bernstein} inequality-based test and the \eqn{\chi^2}{} based test evaluates
what are the significant solutions at a given significance level.
\end{Description}
\begin{Usage}
\begin{verbatim}
Hybrid.testing(sim.matrix, alpha = 0.01, s0 = 0.9)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{sim.matrix}] a matrix that stores the similarity between pairs of clustering across multiple number of clusters
and multiple clusterings.
\item[\code{alpha}] significance level (default 0.01) 
\item[\code{s0}] threshold for the similarity value used in the \eqn{\chi^2}{} based test (default 0.9) 
\end{ldescription}
\end{Arguments}
\begin{Details}\relax
The function accepts as input  a similarity matrix that stores the similarity measure between multiple pairs of clusterings 
considering different number of clusters. Each row of the matrix corresponds to a k-clustering, each column to different repeated 
measures.
Note that the similarities can be computed using different clustering algorithms, different perturbations methods 
(resampling techniques, random projections or noise-injection methods) and different similarity measures. 
The stability index for a given clustering is computed as the mean of the similarity indices between pairs of 
k-clusterings obtained from the perturbed data. The similarity matrix given as input can be obtained from the functions
do.similarity.resampling, do.similarity.projection, do.similarity.noise.
The clusterings are ranked according to the values of the stability indices and the Bernstein inequality-based test is iteratively performed
between the top ranked and upward from the last ranked clustering until the null hypothesis (that is no significant difference between the clustering
solutions) cannot be rejected. Then, to refine the solutions, the chi square-based test is performed on the remaining top ranked clusterings.
The significant solutions at a given \eqn{\alpha}{} significance level, as well as the computed p-values are returned.
\end{Details}
\begin{Value}
a list with 6 elements: 
\begin{ldescription}
\item[\code{n.Bernstein.selected }] number of clusterings selected as  significant by the Bernstein test
\item[\code{n.chi.sq.selected }] number of clusterings selected as  significant by chi square test. It may be 
equal to 0 if Bernstein test selects only 1 clustering.
\item[\code{Bernstein.res }] data frame with the p-values obtained from Bernstein inequality 
\item[\code{chi.sq.res }] data frame with the p-values obtained from chi square test. If through Bernstein 
inequality test only 1 clustering is significant this component is NULL
\item[\code{selected.res }] data frame with the results relative to the clusterings selected by the overall hybrid test 
\item[\code{F}] a list of cumulative distribution functions (of class ecdf) (not sorted).
\end{ldescription}
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valentini@dsi.unimi.it}
\end{Author}
\begin{References}\relax
W. Hoeffding, Probability inequalities for sums of independent random variables, J. Amer. Statist. Assoc. vol.58 pp. 13-30, 1963.

A.Bertoni, G. Valentini, Model order selection for clustered bio-molecular data,  
In: Probabilistic Modeling and Machine Learning in Structural and Systems Biology, J. Rousu, S. Kaski and E. Ukkonen (Eds.), 
Tuusula, Finland, 17-18 June,  2006

A.Bertoni, G. Valentini, Discovering significant structures in clustered data through Bernstein inequality, 
CISI '06, Conferenza Italiana Sistemi Intelligenti, Ancona, Italia, 2006.
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{Bernstein.compute.pvalues}{Bernstein.compute.pvalues}}, \code{\LinkA{Chi.square.compute.pvalues}{Chi.square.compute.pvalues}}, 

\code{\LinkA{Hypothesis.testing}{Hypothesis.testing}}, \code{\LinkA{do.similarity.resampling}{do.similarity.resampling}}, 

\code{\LinkA{do.similarity.projection}{do.similarity.projection}}, \code{\LinkA{do.similarity.noise}{do.similarity.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Generation of a synthetic data set with a three-levels hierarchical structure
M1 <- generate.sample.h2 (n=20, l=20, Delta.h=6, Delta.v=3, sd=0.1)
# building a similarity matrix using resampling methods, considering clusterings 
# from 2 to 15 clusters
S1.HC <- do.similarity.resampling (M1, c=15, nsub=20, f=0.8, s=sFM, 
                                   alg.clust.sim=Hierarchical.sim.resampling)
# Application of the Hybrid statistical test
l1.HC <- Hybrid.testing(S1.HC, alpha=0.01, s0=0.95)
# 3 clusterings are selected, according to the hierarchical structure of the data:
l1.HC$selected.res
\end{ExampleCode}
\end{Examples}

\HeaderA{Hypothesis.testing}{Function to select significant clusterings from a given set of p-values}{Hypothesis.testing}
\keyword{cluster}{Hypothesis.testing}
\keyword{htest}{Hypothesis.testing}
\begin{Description}\relax
For a given set of p-values returned from a given hypothesis testing, it returns the items for which there is no significant difference at
alpha significance level (that is the items for which p > alpha).
\end{Description}
\begin{Usage}
\begin{verbatim}
Hypothesis.testing(d, alpha = 0.01)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{d}] data frame with the p-values returned by a given test of hypothesis (e.g. Bernstein or Chi square-based tests) 
\item[\code{alpha}] significance level 
\end{ldescription}
\end{Arguments}
\begin{Value}
a data frame corresponding to the clusterings significant at alpha level
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valentini@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Bernstein.compute.pvalues}{Bernstein.compute.pvalues}} \code{\LinkA{Chi.square.compute.pvalues}{Chi.square.compute.pvalues}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=1000, d=3, s=0.2)
nsubsamples <- 10;  # number of pairs od clusterings to be evaluated
max.num.clust <- 6; # maximum number of cluster to be evaluated
fract.resampled <- 0.8; # fraction of samples to subsampled
# building a similarity matrix using resampling methods, considering clusterings 
# from 2 to 10 clusters with the k-means algorithm
Sr.Kmeans.sample6 <- do.similarity.resampling(M, c=max.num.clust, nsub=nsubsamples, 
                     f=fract.resampled, s=sFM, alg.clust.sim=Kmeans.sim.resampling);
# computing p-values according to the chi square-based test
dr.Kmeans.sample6 <- Chi.square.compute.pvalues(Sr.Kmeans.sample6);
# test of hypothesis based on the obtained set of p-values
hr.Kmeans.sample6 <- Hypothesis.testing(dr.Kmeans.sample6, alpha=0.01);
# at the given significance level (0.01) the clustering with 2 clusters is selected:
hr.Kmeans.sample6
\end{ExampleCode}
\end{Examples}

\HeaderA{Intersect}{Function to compute the intersection between elements of two vectors}{Intersect}
\keyword{cluster}{Intersect}
\begin{Description}\relax
Having as input two sets of elements represented by two vectors, the intersection between the two sets is performed and
the corresponding vector is returned.
\end{Description}
\begin{Usage}
\begin{verbatim}
Intersect(sub1, sub2)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{sub1}] first vector representing the first set 
\item[\code{sub2}] second vector representing the second set 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector that stores the elements common to the two input vectors
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{Examples}
\begin{ExampleCode}
# Intesection between two sets of elements represented by vectors
s1 <- 1:10;
s2 <- 3:12;
Intersect(s1, s2)
\end{ExampleCode}
\end{Examples}

\HeaderA{Kmeans.sim.noise}{Function to compute similarity indices using noise injection techniques and kmeans clustering.}{Kmeans.sim.noise}
\keyword{cluster}{Kmeans.sim.noise}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with random noise is computed for a given number of clusters. 
The variance of the added gaussian noise, estimated from the data as the perc percentile of the standard deviations of the input variables,
the percentile itself and the similarity measure can be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Kmeans.sim.noise(X, c = 2, nnoisy = 100, perc = 0.5, s = sFM, 
                 distance = "euclidean", hmethod = NULL)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nnoisy}] number of pairs of noisy data 
\item[\code{perc}] percentile of the standard deviations to be used for the added gaussian noise (def. 0.5) 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] actually only the euclidean distance is available "euclidean" (default) 
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nnoisy)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Kmeans.sim.projection}{Kmeans.sim.projection}}, \code{\LinkA{Kmeans.sim.resampling}{Kmeans.sim.resampling}}, \code{\LinkA{perturb.by.noise}{perturb.by.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Kmeans.sim.noise(M, c = 2, nnoisy = 20,  s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Kmeans.sim.noise(M, c = 3, nnoisy = 20,  s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Kmeans.sim.noise(M, c = 2, nnoisy = 20,  s = sJaccard)
# 2 clusters using 0.95 percentile (more noise)
v095 <- Kmeans.sim.noise(M, c = 2, nnoisy = 20,  s = sFM, perc=0.95)
\end{ExampleCode}
\end{Examples}

\HeaderA{Kmeans.sim.projection}{Function to compute similarity indices using random projections and kmeans clustering.}{Kmeans.sim.projection}
\keyword{cluster}{Kmeans.sim.projection}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with random projections is computed for a given number of clusters. 
The dimension of the projected data, the type of randomized map and the similarity measure may be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Kmeans.sim.projection(X, c = 2, nprojections = 100, dim = 2, pmethod = "PMO", 
  scale = TRUE, seed = 100, s = sFM, distance = "euclidean", hmethod = "ward")
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nprojections}] number of pairs of projected data 
\item[\code{dim}] dimension of the projected data 
\item[\code{pmethod}] projection method. It must be one of the following:  
RS" (random subspace projection)
PMO" (Plus Minus One random projection)
Norm" (normal random projection)
Achlioptas" (Achlioptas random projection)
\item[\code{scale}] if TRUE randomized projections are scaled (default) 
\item[\code{seed}] numerical seed for the random generator 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] actually only the euclidean distance is available "euclidean" (default) 
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nprojections)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Kmeans.sim.resampling}{Kmeans.sim.resampling}}, \code{\LinkA{Kmeans.sim.noise}{Kmeans.sim.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Kmeans.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                            pmethod = "PMO", s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Kmeans.sim.projection(M, c = 3, nprojections = 20, dim = 200, 
                            pmethod = "PMO", s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Kmeans.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                             pmethod = "PMO", s = sJaccard)
\end{ExampleCode}
\end{Examples}

\HeaderA{Kmeans.sim.resampling}{Function to compute similarity indices using resampling techniques and kmeans clustering.}{Kmeans.sim.resampling}
\keyword{cluster}{Kmeans.sim.resampling}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with resampling techniques is computed for a given number of clusters,
using the kmeans algorithm.
The fraction of the resampled data (without replacement) and the similarity measure can be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
Kmeans.sim.resampling(X, c = 2, nsub = 100, f = 0.8, s = sFM, 
                      distance = "euclidean", hmethod = NULL)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nsub}] number of subsamples 
\item[\code{f}] fraction of the data resampled without replacement 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] actually only the euclidean distance is available "euclidean" (default) 
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nsub)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{Kmeans.sim.projection}{Kmeans.sim.projection}}, \code{\LinkA{Kmeans.sim.noise}{Kmeans.sim.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- Kmeans.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- Kmeans.sim.resampling(M, c = 3, nsub = 20, f = 0.8, s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- Kmeans.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sJaccard)
\end{ExampleCode}
\end{Examples}

\HeaderA{PAM.sim.noise}{Function to compute similarity indices using noise injection techniques and PAM clustering.}{PAM.sim.noise}
\keyword{cluster}{PAM.sim.noise}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with random noise is computed for a given number of clusters. 
The variance of the added gaussian noise, estimated from the data as the perc percentile of the standard deviations of the input variables,
the percentile itself and the similarity measure can be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
PAM.sim.noise(X, c = 2, nnoisy = 100, perc = 0.5, s = sFM, 
              distance = "euclidean", hmethod = NULL)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nnoisy}] number of pairs of noisy data 
\item[\code{perc}] percentile of the standard deviations to be used for the added gaussian noise (def. 0.5) 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nnoisy)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{PAM.sim.projection}{PAM.sim.projection}}, \code{\LinkA{PAM.sim.resampling}{PAM.sim.resampling}}, \code{\LinkA{perturb.by.noise}{perturb.by.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- PAM.sim.noise(M, c = 2, nnoisy = 20,  s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- PAM.sim.noise(M, c = 3, nnoisy = 20,  s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- PAM.sim.noise(M, c = 2, nnoisy = 20,  s = sJaccard)
# 2 clusters using 0.95 percentile (more noise)
v095 <- PAM.sim.noise(M, c = 2, nnoisy = 20,  s = sFM, perc=0.95)
\end{ExampleCode}
\end{Examples}

\HeaderA{PAM.sim.projection}{Function to compute similarity indices using random projections and PAM clustering.}{PAM.sim.projection}
\keyword{cluster}{PAM.sim.projection}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with random projections is computed for a given number of clusters. 
The dimension of the projected data, the type of randomized map and the similarity measure may be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
PAM.sim.projection(X, c = 2, nprojections = 100, dim = 2, pmethod = "PMO", 
  scale = TRUE, seed = 100, s = sFM, distance = "euclidean", hmethod = NULL)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nprojections}] number of pairs of projected data 
\item[\code{dim}] dimension of the projected data 
\item[\code{pmethod}] projection method. It must be one of the following:  
RS" (random subspace projection)
PMO" (Plus Minus One random projection)
Norm" (normal random projection)
Achlioptas" (Achlioptas random projection)
\item[\code{scale}] if TRUE randomized projections are scaled (default) 
\item[\code{seed}] numerical seed for the random generator 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation)
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nprojections)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{PAM.sim.resampling}{PAM.sim.resampling}}, \code{\LinkA{PAM.sim.noise}{PAM.sim.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- PAM.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                         pmethod = "PMO", s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- PAM.sim.projection(M, c = 3, nprojections = 20, dim = 200, 
                         pmethod = "PMO", s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- PAM.sim.projection(M, c = 2, nprojections = 20, dim = 200, 
                          pmethod = "PMO", s = sJaccard)
\end{ExampleCode}
\end{Examples}

\HeaderA{PAM.sim.resampling}{Function to compute similarity indices using resampling techniques and PAM clustering.}{PAM.sim.resampling}
\keyword{cluster}{PAM.sim.resampling}
\begin{Description}\relax
A vector of similarity measures between pairs of clusterings perturbed with resampling techniques is computed for a given number of clusters,
using the PAM algorithm.
The fraction of the resampled data (without replacement) and the similarity measure can be selected.
\end{Description}
\begin{Usage}
\begin{verbatim}
PAM.sim.resampling(X, c = 2, nsub = 100, f = 0.8, s = sFM, 
                   distance = "euclidean", hmethod = NULL)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{c}] number of clusters 
\item[\code{nsub}] number of subsamples 
\item[\code{f}] fraction of the data resampled without replacement 
\item[\code{s}] similarity function to be used. It may be one of the following: 
- sFM (Fowlkes and Mallows)
- sJaccard (Jaccard)
- sM (matching coefficient)
(default Fowlkes and Mallows) 
\item[\code{distance}] it must be one of the two: "euclidean" (default) or "pearson" (that is 1 - Pearson correlation) 
\item[\code{hmethod}] parameter used for internal compatibility. 
\end{ldescription}
\end{Arguments}
\begin{Value}
vector of the computed similarity measures (length equal to nsub)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{PAM.sim.projection}{PAM.sim.projection}}, \code{\LinkA{PAM.sim.noise}{PAM.sim.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation
M <- generate.sample6 (n=20, m=10, dim=600, d=3, s=0.2);
# computing a vector of similarity indices with 2 clusters:
v2 <- PAM.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sFM)
# computing a vector of similarity indices with 3 clusters:
v3 <- PAM.sim.resampling(M, c = 3, nsub = 20, f = 0.8, s = sFM)
# computing a vector of similarity indices with 2 clusters using the Jaccard index
v2J <- PAM.sim.resampling(M, c = 2, nsub = 20, f = 0.8, s = sJaccard)
\end{ExampleCode}
\end{Examples}

\HeaderA{perturb.by.noise}{Function to generate a data set perturbed by noise.}{perturb.by.noise}
\keyword{cluster}{perturb.by.noise}
\begin{Description}\relax
This funtion adds gaussian noise to the data. The mean of the gaussian noise is 0 and the standard deviation is estimated from the data.
The gaussian noise  added to the data has 0 mean and the standard deviation is estimated from the data (it is 
set to a given percentile value of the standard deviations computed for each variable).
\end{Description}
\begin{Usage}
\begin{verbatim}
perturb.by.noise(X, perc = 0.5)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] matrix of data (variables are rows, examples columns) 
\item[\code{perc}] percentile of the standard deviation (def: 0.5) 
\end{ldescription}
\end{Arguments}
\begin{Value}
matrix of perturbed data (variables are rows, examples columns)
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{References}\relax
McShane, L.M., Radmacher, D., Freidlin, B., Yu, R.,  Li, M.C. and Simon, R.,
Method for assessing reproducibility of clustering patterns observed in analyses of microarray data,
Bioinformatics, 11(8), pp. 1462-1469, 2002.
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{do.similarity.noise}{do.similarity.noise}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=100, d=3, s=0.2);
# genearation of a data set perturbed by noise
M.perturbed <- perturb.by.noise(M);
# genearation of a data set more perturbed by noise
M.more.perturbed <- perturb.by.noise(M, perc=0.95);
\end{ExampleCode}
\end{Examples}

\HeaderA{plot.cumulative}{Function to plot the empirical cumulative distribution function of the similarity values}{plot.cumulative}
\methaliasA{plot.cumulative.multiple}{plot.cumulative}{plot.cumulative.multiple}
\keyword{cluster}{plot.cumulative}
\begin{Description}\relax
The function \code{plot.cumulative} plots the ecdf of the similarity values between pairs of clusterings for a specific number of clusters.
The function \code{plot.cumulative.multiple} plots the graphs of the empirical cumulative distributions corresponding to different number of clusters, 
using different patterns and/or different colors for each graph. Up to 15  ecdf graphs can be plotted simultaneously.
\end{Description}
\begin{Usage}
\begin{verbatim}
plot.cumulative(F)

plot.cumulative.multiple(list.F, labels = NULL, min.x = -1, colors = TRUE)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{F}] Function of class ecdf  that stores the discrete values of the cumulative distribution 
\item[\code{list.F}] a list of function of class ecdf 
\item[\code{labels}] vector of the labels associated to the CDF. If  NULL (default), then a vector of labels from 2 to lenght(list.F)+1
is used.
\item[\code{min.x}] minimum value to be plotted for similarities. If -1 (default) the minimum of the similarity value is obtained
from list.F 
\item[\code{colors}] if TRUE (default) different colors are used to plot the different ECDF, otherwise black lines are used 
\end{ldescription}
\end{Arguments}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{compute.cumulative.multiple}{compute.cumulative.multiple}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=1000, d=3, s=0.2);
# generation of multiple similarity measures by resampling
Sr.kmeans.sample6 <- do.similarity.resampling(M, c=10, nsub=20, f=0.8, s=sFM, 
                                      alg.clust.sim=Kmeans.sim.resampling); 
# computation of multiple ecdf (from 2 to 10 clusters)
list.F <- compute.cumulative.multiple(Sr.kmeans.sample6);
# values of the ecdf for 8 clusters 
l <- cumulative.values(list.F[[7]])
# plot of the ecdf for 8 clusters
plot.cumulative(list.F[[7]])
# plot of the empirical cumulative distributions from 2 to 10 clusters
plot.cumulative.multiple(list.F)
\end{ExampleCode}
\end{Examples}

\HeaderA{plot.histograms.similarity}{Plotting histograms of similarity measures between clusterings}{plot.histograms.similarity}
\aliasA{plot.hist.similarity}{plot.histograms.similarity}{plot.hist.similarity}
\aliasA{plot.multiple.hist.similarity}{plot.histograms.similarity}{plot.multiple.hist.similarity}
\keyword{cluster}{plot.histograms.similarity}
\begin{Description}\relax
These functions plot histograms of a set of similarity measures obtained through perturbation methods.
In particular \code{plot.hist.similarity} plots a single histogram referred to a specific number of clusters,
while \code{plot.multiple.hist.similarity} plots multiple histograms referred to different numbers of clusters
(one for each number of clusters, i.e. one for each row of
of the matrix \eqn{S}{} of similarity values).
\end{Description}
\begin{Usage}
\begin{verbatim}
plot.hist.similarity(sim, nbins = 25)

plot.multiple.hist.similarity(S, n.col = 3, labels = NULL, nbins = 25)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{sim}] vector of similarity values 
\item[\code{nbins}] number of the bins of the histogram 
\item[\code{S}] Matrix of similarity values, rows correspond to diferent number of clusters 
\item[\code{n.col}] number of columns in the grid of the histograms (default = 3 
\item[\code{labels}] label of the histograms. If NULL (default) the number of clusters from 2 to nrow(S)+1 are used 
\end{ldescription}
\end{Arguments}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{plot.cumulative}{plot.cumulative}},  \code{\LinkA{plot.cumulative.multiple}{plot.cumulative.multiple}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=1000, d=3, s=0.2);
# generation of multiple similarity measures by resampling
Sr.kmeans.sample6 <- do.similarity.resampling(M, c=10, nsub=20, f=0.8, s=sFM, 
                                      alg.clust.sim=Kmeans.sim.resampling); 
# plot of the histograms of similarity measures for clusterings from 2 to 10 clusters:
plot.multiple.hist.similarity (Sr.kmeans.sample6, n.col=3, labels=NULL, nbins=25);
# the same as postrcript file
postscript(file="histograms.eps", horizontal=FALSE, onefile = FALSE);
oldpar <- par(cex=4.0, mex=1);
plot.multiple.hist.similarity (Sr.kmeans.sample6, n.col=3, labels=NULL, nbins=25);
dev.off();
# plot of a single histogram
plot.hist.similarity(Sr.kmeans.sample6[2,], nbins = 25)
\end{ExampleCode}
\end{Examples}

\HeaderA{plot.pvalues}{Function to plot p-values for different tests of hypothesis}{plot.pvalues}
\keyword{cluster}{plot.pvalues}
\keyword{htest}{plot.pvalues}
\begin{Description}\relax
The p-values corresponding to different k-clusterings according to different hypothesis testing are plotted. A horizontal line corresponding to
a given alpha value (significance) is also plotted. In the x axis is represented the number of clusters sorted according to the value of the stability index,
and in the y axis the corresponding p-value.
In this way the results of different tests of hypothesis can be compared.
\end{Description}
\begin{Usage}
\begin{verbatim}
plot.pvalues(l,alpha=1e-02,legendy=0, leg_label=NULL, colors=TRUE)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{l}] a list of lists. Each component list represents a different test of hypothesis, and it has in turn 4 components:              
ordered.clusterings : a vector with the number of clusters ordered from the most significant to the least significant;
p.value : a vector with the corresponding p-values computed according to chi-square test between multiple proportions
in descending order (their values correspond to the clusterings of the vector ordered.clusterings);
means : vector with the mean similarity (stability index) for each clustering;
variance : vector with the variance of the similarity for each clustering.
\item[\code{alpha}] alpha value for which the straight line is plotted 
\item[\code{legendy}] ordinate of the legend. If 0 (def.) no legend is plotted. 
\item[\code{leg\_label}] labels of the legend. If NULL (def.) the text "test 1, test 2, ... test n" for the n tests is printed.
Otherwise it is a vector of characters specifying the text to be printed
\item[\code{colors}] if TRUE (def.) lines are printed with colors, otherwise using only different line pattern
\end{ldescription}
\end{Arguments}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{SeeAlso}\relax
\code{\LinkA{plot.cumulative.multiple}{plot.cumulative.multiple}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Data set generation
M <- generate.sample6 (n=20, m=10, dim=1000, d=3, s=0.2);
# generation of multiple similarity measures by resampling
Sr.kmeans.sample6 <- do.similarity.resampling(M, c=10, nsub=20, f=0.8, s=sFM, 
                                      alg.clust.sim=Kmeans.sim.resampling); 
# hypothesis testing using the chi-square based test
d.chi <- Chi.square.compute.pvalues(Sr.kmeans.sample6)
# hypothesis testing using the Bernstein based test
d.Bern <- Bernstein.compute.pvalues(Sr.kmeans.sample6)
# hypothesis testing using the Bernstein based test (with independence assumption)
d.Bern.ind <- Bernstein.ind.compute.pvalues(Sr.kmeans.sample6)
l <- list(d.chi, d.Bern, d.Bern.ind);
# plot of the corresponding computed p-values
plot.pvalues(l, alpha = 1e-05, legendy = 1e-12)
\end{ExampleCode}
\end{Examples}

\HeaderA{Similarity.measures}{Similarity measures between pairs of clusterings}{Similarity.measures}
\aliasA{sFM}{Similarity.measures}{sFM}
\aliasA{sJaccard}{Similarity.measures}{sJaccard}
\aliasA{sM}{Similarity.measures}{sM}
\keyword{cluster}{Similarity.measures}
\begin{Description}\relax
Classical similarity measures between pairs of clusterings are implememted. These measures use the pairwise boolean membership matrix 
(\code{\LinkA{Do.boolean.membership.matrix}{Do.boolean.membership.matrix}}) to compute the similarity between two clusterings, using the matrix as a vector and computing
the result as an internal product. It may be shown that the same result may be obtained using contingency matrices and the classical
definition of Fowlkes and Mallows (implemented with the function \code{sFM}), Jaccard (implemented with the function \code{sJaccard}) 
and Matching (Rand Index, implemented with the function \code{sM}) coefficients.
Their values range from 0 to 1 (0 no similarity, 1 identity).
\end{Description}
\begin{Usage}
\begin{verbatim}
sFM(M1, M2)
sJaccard(M1, M2)
sM(M1, M2)
\end{verbatim}
\end{Usage}
\begin{Arguments}
\begin{ldescription}
\item[\code{M1}] boolean membership matrix representing the first clustering 
\item[\code{M2}] boolean membership matrix representing the second clustering 
\end{ldescription}
\end{Arguments}
\begin{Value}
similarity measure between the two clusterings according to Fowlkes and Mallows (\code{sFM}), Jaccard (\code{sJaccard}) and
Matching (\code{sM}) coefficients.
\end{Value}
\begin{Author}\relax
Giorgio Valentini \email{valenti@dsi.unimi.it}
\end{Author}
\begin{References}\relax
Ben-Hur, A. Ellisseeff, A. and Guyon, I., A stability based method for discovering structure in clustered data,
In: "Pacific Symposium on Biocomputing", Altman, R.B. et al (eds.), pp, 6-17, 2002.
\end{References}
\begin{SeeAlso}\relax
\code{\LinkA{Do.boolean.membership.matrix}{Do.boolean.membership.matrix}}
\end{SeeAlso}
\begin{Examples}
\begin{ExampleCode}
# Synthetic data set generation (3 clusters with 20 examples for each cluster)
M <- generate.sample3(n=20, m=2)
# k-means clustering with 3 clusters
r1<-kmeans(t(M), c=3, iter.max = 1000);
# this function is implemented in the clusterv package:
cl1 <- Transform.vector.to.list(r1$cluster); 
# generation of a boolean membership square matrix:
Bkmeans <- Do.boolean.membership.matrix(cl1, 60, 1:60)
# the same as above, using PAM clustering with 3 clusters
d <- dist (t(M));
r2 <- pam (d,3,cluster.only=TRUE);
cl2 <- Transform.vector.to.list(r2);
BPAM <- Do.boolean.membership.matrix(cl2, 60, 1:60)
# computation of the Fowlkes and Mallows index beween the k-means and the PAM clustering:
sFM(Bkmeans, BPAM)
# computation of the Jaccard index beween the k-means and the PAM clustering:
sJaccard(Bkmeans, BPAM)
# computation of the Matching coefficient beween the k-means and the PAM clustering:
sM(Bkmeans, BPAM)
\end{ExampleCode}
\end{Examples}

\end{document}
